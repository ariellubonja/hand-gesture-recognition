{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "If on Colab, upload the datasets/ and utils/ folders"
      ],
      "metadata": {
        "id": "qDL8KiwuAMJj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "m_VjIeHe-4nF"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt # graphing \n",
        "\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnjzdyU4AoJU",
        "outputId": "390bec17-a261-43f2-854a-83fa8af59eaf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "202303312004_bestmodel.pt  datasets  sample_data  utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJUHQnF9-4nJ",
        "outputId": "e70a9e68-d620-4541-e5c5-b9b7ab8a2fe0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<HDF5 file \"Signs_Data_Training.h5\" (mode r)>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Opening H5 file\n",
        "import h5py # https://www.h5py.org\n",
        "\n",
        "training_data = h5py.File(\"./datasets/Signs_Data_Training.h5\", \"r\")\n",
        "training_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI_vJh3G-4nL",
        "outputId": "415a4ee9-fd2c-4c69-e2e8-d35815cfc03a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KeysViewHDF5 ['list_classes', 'train_set_x', 'train_set_y']>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "training_data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "yDqGcJZRB3C-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = 'cpu'"
      ],
      "metadata": {
        "id": "m5dGaZ95PwZX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbvlbbTr-4nM",
        "outputId": "a37bca2a-4461-4f60-c99e-45823d80917c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape:  torch.Size([1080, 64, 64, 3])\n"
          ]
        }
      ],
      "source": [
        "X_train = np.array(training_data['train_set_x'])\n",
        "y_train = np.array(training_data['train_set_y'])\n",
        "\n",
        "# train_data = torch.tensor(training_data['train_set_x']) # This is really slow\n",
        "train_data = torch.tensor(X_train)\n",
        "# train_labels = torch.tensor(training_data['train_set_y']) # This is really slow\n",
        "train_labels = torch.tensor(y_train)\n",
        "print(\"Train data shape: \", train_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqq5v5Dd-4nN"
      },
      "source": [
        "#### 1080 photos of 64x64 size, 3 colors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsS7uuvh-4nO"
      },
      "source": [
        "### Transpose data into the format Torch wants - (n_channels, width, height)\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ioUYtqef-4nP"
      },
      "outputs": [],
      "source": [
        "# # https://numpy.org/doc/stable/reference/generated/numpy.transpose.html\n",
        "\n",
        "# train_data = train_data.view((100, 3, 64, 64))\n",
        "\n",
        "\n",
        "# train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jUlOt8wK-4nQ"
      },
      "outputs": [],
      "source": [
        "# Taken from Yann's paper\n",
        "class LeNet5(torch.nn.Module):\n",
        "    def __init__(self, input_height, input_width, n_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # C1. Put padding bcs MNIST had 32x32 data. We have 28x28\n",
        "        self.C1 = torch.nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, padding=2)\n",
        "        #feature extraction of 5x5 kernels\n",
        "    \n",
        "        # S2\n",
        "        self.S2 = torch.nn.AvgPool2d(kernel_size=2)\n",
        "        #reduce dimensions of feature maps while preserving info\n",
        "        \n",
        "        \n",
        "        # C3\n",
        "        self.C3 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
        "        #feature extraction of 5x5 kernels\n",
        "\n",
        "        # S4\n",
        "        self.S4 = torch.nn.AvgPool2d(kernel_size=4) # Edited this due to our data being 64x64\n",
        "        #reduce dimensions of feature maps while preserving info\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        # C5\n",
        "        self.C5 = torch.nn.Conv2d(in_channels=16, out_channels=120, kernel_size=7) # Edited this due to our data being 64x64\n",
        "        #feature extraction of 5x5 kernels\n",
        "\n",
        "        # F6\n",
        "        self.F6 = torch.nn.Linear(in_features= 120, out_features=84)\n",
        "        # self.layer4 = torch.nn.Conv2d(in_channels=64, out_channels=16, kernel_size=1)\n",
        "        #neuron layer\n",
        "\n",
        "        # F7\n",
        "        self.F7 = torch.nn.Linear(in_features= 84, out_features=n_classes)\n",
        "        #neuron layer\n",
        "        \n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        ### forward pass module\n",
        "        # x = torch.unsqueeze(x, 1)\n",
        "\n",
        "        ### feed forward function\n",
        "#         print(\"x.shape: \", x.shape)\n",
        "        \n",
        "        # PyTorch expects (Batch_size, channel_nr, width, height)\n",
        "        \n",
        "        l1out = self.C1(x)\n",
        "        l1out = torch.nn.Tanh()(l1out)\n",
        "        \n",
        "        # print(\"l1out.shape: After C1 and TanH\", l1out.shape)\n",
        "\n",
        "        # AvgPooling\n",
        "        l2out = self.S2(l1out)\n",
        "        \n",
        "        # print(\"l2out.shape: after AvgPooling \", l2out.shape)\n",
        "\n",
        "        l3out = self.C3(l2out)\n",
        "        l3out = torch.nn.Tanh()(l3out)\n",
        "        \n",
        "        # print(\"l3out.shape: After C3 and TanH\", l3out.shape)\n",
        "\n",
        "        # AvgPooling\n",
        "        l4out = self.S4(l3out)\n",
        "        \n",
        "        # print(\"l4out.shape, After Avg Pooling: \", l4out.shape)\n",
        "\n",
        "        l5out = self.C5(l4out)\n",
        "        l5out = torch.nn.Tanh()(l5out)\n",
        "        l5out = np.squeeze(l5out)\n",
        "        # print(\"l5out.shape: after C5 and Tanh: \", l5out.shape)\n",
        "        \n",
        "        l5out = torch.nn.Dropout()(l5out)\n",
        "        \n",
        "        l6out = self.F6(l5out) # squeeze bcs (120,1) vector to (120,) - a numpy broadcast thing\n",
        "        l6out = torch.nn.Tanh()(l6out)\n",
        "        # print(\"l6out.shape: after F6 and squeeze \", l6out.shape)\n",
        "        \n",
        "        l7out = self.F7(l6out)\n",
        "        # print(\"l7out.shape: After F7 \", l7out.shape)\n",
        "\n",
        "        return l7out\n",
        "    \n",
        "    \n",
        "#feature extraction C1\n",
        "#activation with Tanh\n",
        "#Pooling (reduce dimensions) S2\n",
        "\n",
        "#feature extraction C3\n",
        "#activation with Tanh\n",
        "#pooling (reduce dimensions) S4\n",
        "\n",
        "#feature extraction C5\n",
        "#activatiton with Tanh\n",
        "\n",
        "#Linear squeeze \n",
        "#activation with Tanh\n",
        "\n",
        "#Linear squeeze to outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4V-eaEc-4nS"
      },
      "source": [
        "## Split Train into Train and Dev. Test set is in another HDF5 file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZdPCiSh0-4nS"
      },
      "outputs": [],
      "source": [
        "# Create Cross-Validation/Dev set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_dev, y_train, y_dev = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEo_mNQ9-4nT",
        "outputId": "62447745-daa2-4fe2-f697-83fd3b6910c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train.shape:  torch.Size([864, 64, 64, 3])\n",
            "y_train.shape:  torch.Size([864])\n"
          ]
        }
      ],
      "source": [
        "print(\"X_train.shape: \", X_train.shape)\n",
        "print(\"y_train.shape: \", y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XQQxpBft-4nU"
      },
      "outputs": [],
      "source": [
        "MODE = \"train\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2RHeXZPe-4nU"
      },
      "outputs": [],
      "source": [
        "# https://towardsdatascience.com/installing-pytorch-on-apple-m1-chip-with-gpu-acceleration-3351dc44d67c\n",
        "\n",
        "# this ensures that the current MacOS version is at least 12.3+\n",
        "#print(torch.backends.mps.is_available())\n",
        "\n",
        "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
        "#print(torch.backends.mps.is_built())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8d__kPsk-4nV"
      },
      "outputs": [],
      "source": [
        "# torch.device(\"mps\") # Disable this if not on apple silicon!"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mOwxkRCuPrDz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_IMAGES = X_train.to(device)\n",
        "TRAIN_LABELS = y_train.to(device)\n",
        "DEV_IMAGES = X_dev.to(device)\n",
        "DEV_LABELS = y_dev.to(device)"
      ],
      "metadata": {
        "id": "f2UVFdKLFjQl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_IMAGES.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNQjsoGxLQMw",
        "outputId": "25dd1509-e1f4-49fa-9e91-1bbd1e8b7471"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_IMAGES.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKw4VH_aQatA",
        "outputId": "645e1802-bd89-4c59-c4cf-3dc614eda42a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([864, 64, 64, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_IMAGES = TRAIN_IMAGES.view(len(TRAIN_IMAGES), 3, 64, 64)\n",
        "DEV_IMAGES = DEV_IMAGES.view(len(DEV_IMAGES), 3, 64, 64)"
      ],
      "metadata": {
        "id": "G43CZmbqQdrS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuIQKyBY-4nW",
        "outputId": "782eedc1-be33-4e21-8ddf-72bc8144fbc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On step 0:\tTrain loss 1.7913676500320435\t|\tDev acc is 0.125\n",
            "On step 100:\tTrain loss 1.0210460424423218\t|\tDev acc is 0.2222222238779068\n",
            "On step 200:\tTrain loss 0.6570978760719299\t|\tDev acc is 0.35185185074806213\n",
            "On step 300:\tTrain loss 0.5518596768379211\t|\tDev acc is 0.31481480598449707\n",
            "On step 400:\tTrain loss 0.37281909584999084\t|\tDev acc is 0.33796295523643494\n",
            "On step 500:\tTrain loss 0.3422926068305969\t|\tDev acc is 0.32870370149612427\n",
            "On step 600:\tTrain loss 0.25629812479019165\t|\tDev acc is 0.28703704476356506\n",
            "On step 700:\tTrain loss 0.26120829582214355\t|\tDev acc is 0.3333333432674408\n",
            "On step 800:\tTrain loss 0.19605153799057007\t|\tDev acc is 0.3240740895271301\n",
            "On step 900:\tTrain loss 0.17533229291439056\t|\tDev acc is 0.3240740895271301\n",
            "On step 1000:\tTrain loss 0.07086658477783203\t|\tDev acc is 0.2916666567325592\n",
            "On step 1100:\tTrain loss 0.16020970046520233\t|\tDev acc is 0.2916666567325592\n",
            "On step 1200:\tTrain loss 0.07286419719457626\t|\tDev acc is 0.3055555522441864\n",
            "On step 1300:\tTrain loss 0.057283345609903336\t|\tDev acc is 0.29629629850387573\n",
            "On step 1400:\tTrain loss 0.088204525411129\t|\tDev acc is 0.2777777910232544\n",
            "On step 1500:\tTrain loss 0.036531757563352585\t|\tDev acc is 0.26851850748062134\n",
            "On step 1600:\tTrain loss 0.021539656445384026\t|\tDev acc is 0.31018519401550293\n",
            "On step 1700:\tTrain loss 0.05253051221370697\t|\tDev acc is 0.29629629850387573\n",
            "On step 1800:\tTrain loss 0.035577114671468735\t|\tDev acc is 0.30092594027519226\n",
            "On step 1900:\tTrain loss 0.010609331540763378\t|\tDev acc is 0.2916666567325592\n",
            "Training completed, saving model at {model_savepath}\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "from utils.accuracies import dev_acc_and_loss, approx_train_acc_and_loss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Modified From Ariel's EN.601.675 HW3\n",
        "        \n",
        "LOG_DIR = \".\"\n",
        "MODEL_SAVE_DIR = \".\"\n",
        "LEARNING_RATE = 0.0004 #arguments.get('lr')\n",
        "BATCH_SIZE = 100 # 250 #arguments.get('bs')\n",
        "EPOCHS = 2000 # arguments.get('epochs')\n",
        "DATE_PREFIX = datetime.datetime.now().strftime('%Y%m%d%H%M')\n",
        "\n",
        "### format dataset to the appropriate shape/dimensions necessary to be input into the model\n",
        "\n",
        "\n",
        "n_train_imgs = TRAIN_IMAGES.shape[0]\n",
        "HEIGHT = TRAIN_IMAGES.shape[1]\n",
        "WIDTH = TRAIN_IMAGES.shape[2]\n",
        "#  this will not be correct if not all classes are present in training\n",
        "#   But if classes are entirely missing from training, we cannot possibly hope to do well on them\n",
        "N_CLASSES = len(torch.unique(TRAIN_LABELS))\n",
        "# raise NotImplementedError\n",
        "\n",
        "### Normalize \n",
        "train_imgs = TRAIN_IMAGES - torch.mean(torch.Tensor.float(TRAIN_IMAGES))/ torch.std(torch.Tensor.float(TRAIN_IMAGES))\n",
        "dev_imgs = (DEV_IMAGES - torch.mean(torch.Tensor.float(DEV_IMAGES)))/ torch.std(torch.Tensor.float(DEV_IMAGES))\n",
        "\n",
        "### change depending on your model's instantiation\n",
        "\n",
        "model = LeNet5(input_height = HEIGHT, input_width= WIDTH,\n",
        "                  n_classes=N_CLASSES)\n",
        "if device != 'cpu':\n",
        "    model.cuda()\n",
        "\n",
        "\n",
        "### (OPTIONAL) : you can change the choice of optimizer here if you wish.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "train_acc_col = []\n",
        "train_loss_col = []\n",
        "dev_acc_col = []\n",
        "dev_loss_col = []\n",
        "steps_col = []\n",
        "for step in range(EPOCHS):\n",
        "    i = np.random.choice(train_imgs.shape[0], size=BATCH_SIZE, replace=False)\n",
        "    x = train_imgs[i]#).astype(np.float32))\n",
        "    y = TRAIN_LABELS[i]#.astype(int)))\n",
        "\n",
        "    # x = x.unsqueeze(0)\n",
        "\n",
        "    # Forward pass: Get logits for x\n",
        "    logits = model(x.view((100, 3, 64, 64)))\n",
        "    # Compute loss\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_acc, train_loss = approx_train_acc_and_loss(model, train_imgs, TRAIN_LABELS)\n",
        "    dev_acc, dev_loss = dev_acc_and_loss(model, dev_imgs, DEV_LABELS)\n",
        "    train_acc_col.append(train_acc)\n",
        "    train_loss_col.append(train_loss)\n",
        "    dev_acc_col.append(dev_acc)\n",
        "    dev_loss_col.append(dev_loss)\n",
        "    steps_col.append(step)\n",
        "    # log model performance every 100 epochs\n",
        "    if step % 100 == 0:\n",
        "      \n",
        "        step_metrics = {\n",
        "            'step': step, \n",
        "            'train_loss': loss.item(), \n",
        "            'train_acc': train_acc,\n",
        "            'dev_loss': dev_loss,\n",
        "            'dev_acc': dev_acc\n",
        "        }\n",
        "\n",
        "        print(f\"On step {step}:\\tTrain loss {train_loss}\\t|\\tDev acc is {dev_acc}\")\n",
        "        # logger.writerow(step_metrics)\n",
        "# LOGFILE.close()\n",
        "\n",
        "# Save model\n",
        "model_savepath = os.path.join(MODEL_SAVE_DIR,f\"{DATE_PREFIX}_bestmodel.pt\")\n",
        "\n",
        "\n",
        "print(\"Training completed, saving model at {model_savepath}\")\n",
        "torch.save(model, model_savepath)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(model.parameters()).device # Where is model on?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM9hLVdSJusI",
        "outputId": "17925a7f-f3c4-4771-b6a4-1368294bba58"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.reshape(x,(0, 3, 1, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "45tiJ6JOJhLJ",
        "outputId": "74a238f5-1c1e-4e15-a8e3-382be79da997"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f5372c60c91d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[0, 3, 1, 2]' is invalid for input of size 1228800"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jCL1mH_-4nX"
      },
      "outputs": [],
      "source": [
        "fig1 = plt.figure()\n",
        "plt.plot(steps_col, train_acc_col, label = \"Training Accuracy\")\n",
        "plt.plot(steps_col, dev_acc_col, label = \"Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "fig2 = plt.figure()\n",
        "plt.plot(steps_col, train_loss_col, label = \"Training Loss\")\n",
        "plt.plot(steps_col, dev_loss_col, label = \"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final Stats:\\tTrain loss {train_loss_col[1999]}\\t|\\tValidation Loss {dev_loss_col[1999]}\\t|\\tTrain acc is {train_acc_col[1999]} \\t|\\tDev acc is {dev_acc_col[1999]}\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpcQDoJP-4nY"
      },
      "source": [
        "## Let's see performance on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAicPKsc-4nY"
      },
      "outputs": [],
      "source": [
        "test_data = h5py.File(\"./datasets/Signs_Data_Testing.h5\", \"r\")\n",
        "TEST_IMAGES = test_data[\"test_set_x\"]\n",
        "test_labels = test_data[\"test_set_y\"] # Do not train on these!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEFgf4JX-4nZ"
      },
      "outputs": [],
      "source": [
        "# Transpose into correct shape for PyTorch\n",
        "\n",
        "TEST_IMAGES = np.transpose(TEST_IMAGES, axes=(0, 3, 1, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SomM_LdF-4nZ"
      },
      "outputs": [],
      "source": [
        "TEST_IMAGES.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZoyVolm-4na"
      },
      "outputs": [],
      "source": [
        "MODE = \"test\"\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaoXYF-b-4nb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVQEdjLR-4nb"
      },
      "outputs": [],
      "source": [
        "# model = torch.load(WEIGHTS_FILE) # Load model parameters from file to avoid retraining\n",
        "\n",
        "predictions = []\n",
        "# testSetMean = TEST_IMAGES.mean() # Can't do that - only remove training parameters from it\n",
        "# testSetStd = TEST_IMAGES.std()\n",
        "for test_case in TEST_IMAGES:\n",
        "\n",
        "    ### normalization schemes\n",
        "    test_case = (test_case - np.mean(TRAIN_IMAGES)) / np.std(TRAIN_IMAGES)\n",
        "\n",
        "    # test_case = test_case.reshape(1,28,28)\n",
        "\n",
        "\n",
        "    x = torch.from_numpy(test_case.astype(np.float32))\n",
        "    # x = x.view(1,-1)\n",
        "    logits = model(x.unsqueeze(0))\n",
        "    logits = logits.reshape(-1, 6)\n",
        "    pred = torch.max(logits, 1)[1]\n",
        "    predictions.append(pred.item())\n",
        "\n",
        "predictions[:15] # First 15 predicted labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68FWQRcU-4nc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_4y-zNi-4nd"
      },
      "outputs": [],
      "source": [
        "from utils.accuracies import accuracy\n",
        "\n",
        "print(\"Test accuracy: \", accuracy(np.array(predictions), np.array(test_labels)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iW7rv-I-4nd"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "transform = A.Compose([\n",
        "    A.affinerotate(p = 0.33)\n",
        "    #A.HorizontalFlip(p = 0.5),\n",
        "    #A.ColorJitter(brightness= 0, contrast=(1, 1.5), saturation=(0.5,1), hue=0, p=1)\n",
        "])\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD9xR9jM-4nf"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "X_dev2 = []\n",
        "for num in range(3):\n",
        "    for pos, imageSet in enumerate(X_dev):\n",
        "        merged = cv2.merge([imageSet[2], imageSet[1], imageSet[0]])\n",
        "        image = cv2.cvtColor(merged, cv2.COLOR_BGR2RGB)\n",
        "        image = transform(image=image)['image']\n",
        "        X_dev2.append(image)\n",
        "print('done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGtPeYsH-4ng"
      },
      "outputs": [],
      "source": [
        "for pos, image in enumerate(X_dev2):\n",
        "    plt.figure()\n",
        "    plt.title(pos)\n",
        "    plt.imshow(image)\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5KvAsFR-4nh"
      },
      "outputs": [],
      "source": [
        "for pos, imageSet in enumerate(X_dev):\n",
        "    if pos == 87:\n",
        "        merged = cv2.merge([imageSet[2], imageSet[1], imageSet[0]])\n",
        "        image = cv2.cvtColor(merged, cv2.COLOR_BGR2RGB)\n",
        "        plt.figure()\n",
        "        plt.title(pos)\n",
        "        plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DejP_-mJ-4ni"
      },
      "outputs": [],
      "source": [
        "print(y_dev[87])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd0ixj9x-4nj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}