{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDL8KiwuAMJj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If on Colab, upload the datasets/ and utils/ folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "m_VjIeHe-4nF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # graphing \n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TnjzdyU4AoJU",
    "outputId": "390bec17-a261-43f2-854a-83fa8af59eaf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202303041923_bestmodel.pt\r\n",
      "202303060950_bestmodel.pt\r\n",
      "\u001B[34mdatasets\u001B[m\u001B[m\r\n",
      "finger-gesture-detection-computer-vision-project.ipynb\r\n",
      "finger_gesture_detection_computer_vision_project.ipynb\r\n",
      "\u001B[34mlog_files\u001B[m\u001B[m\r\n",
      "\u001B[34mmodel_files\u001B[m\u001B[m\r\n",
      "\u001B[34mutils\u001B[m\u001B[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJUHQnF9-4nJ",
    "outputId": "e70a9e68-d620-4541-e5c5-b9b7ab8a2fe0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<HDF5 file \"Signs_Data_Training.h5\" (mode r)>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opening H5 file\n",
    "import h5py # https://www.h5py.org\n",
    "\n",
    "training_data = h5py.File(\"./datasets/Signs_Data_Training.h5\", \"r\")\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mI_vJh3G-4nL",
    "outputId": "415a4ee9-fd2c-4c69-e2e8-d35815cfc03a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<KeysViewHDF5 ['list_classes', 'train_set_x', 'train_set_y']>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yDqGcJZRB3C-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "m5dGaZ95PwZX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QbvlbbTr-4nM",
    "outputId": "a37bca2a-4461-4f60-c99e-45823d80917c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  torch.Size([1080, 64, 64, 3])\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(training_data['train_set_x'])\n",
    "y_train = np.array(training_data['train_set_y'])\n",
    "\n",
    "# train_data = torch.tensor(training_data['train_set_x']) # This is really slow\n",
    "train_data = torch.tensor(X_train)\n",
    "# train_labels = torch.tensor(training_data['train_set_y']) # This is really slow\n",
    "train_labels = torch.tensor(y_train)\n",
    "print(\"Train data shape: \", train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iqq5v5Dd-4nN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1080 photos of 64x64 size, 3 colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsS7uuvh-4nO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Transpose data into the format Torch wants - (n_channels, width, height)\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ioUYtqef-4nP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # https://numpy.org/doc/stable/reference/generated/numpy.transpose.html\n",
    "\n",
    "# train_data = train_data.view((100, 3, 64, 64))\n",
    "\n",
    "\n",
    "# train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jUlOt8wK-4nQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Taken from Yann's paper\n",
    "class LeNet5(torch.nn.Module):\n",
    "    def __init__(self, input_height, input_width, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # C1. Put padding bcs MNIST had 32x32 data. We have 28x28\n",
    "        self.C1 = torch.nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, padding=2)\n",
    "        #feature extraction of 5x5 kernels\n",
    "    \n",
    "        # S2\n",
    "        self.S2 = torch.nn.AvgPool2d(kernel_size=2)\n",
    "        #reduce dimensions of feature maps while preserving info\n",
    "        \n",
    "        \n",
    "        # C3\n",
    "        self.C3 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        #feature extraction of 5x5 kernels\n",
    "\n",
    "        # S4\n",
    "        self.S4 = torch.nn.AvgPool2d(kernel_size=4) # Edited this due to our data being 64x64\n",
    "        #reduce dimensions of feature maps while preserving info\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # C5\n",
    "        self.C5 = torch.nn.Conv2d(in_channels=16, out_channels=120, kernel_size=7) # Edited this due to our data being 64x64\n",
    "        #feature extraction of 5x5 kernels\n",
    "\n",
    "        # F6\n",
    "        self.F6 = torch.nn.Linear(in_features= 120, out_features=84)\n",
    "        # self.layer4 = torch.nn.Conv2d(in_channels=64, out_channels=16, kernel_size=1)\n",
    "        #neuron layer\n",
    "\n",
    "        # F7\n",
    "        self.F7 = torch.nn.Linear(in_features= 84, out_features=n_classes)\n",
    "        #neuron layer\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ### forward pass module\n",
    "        # x = torch.unsqueeze(x, 1)\n",
    "\n",
    "        ### feed forward function\n",
    "#         print(\"x.shape: \", x.shape)\n",
    "        \n",
    "        # PyTorch expects (Batch_size, channel_nr, width, height)\n",
    "        \n",
    "        l1out = self.C1(x)\n",
    "        l1out = torch.nn.Tanh()(l1out)\n",
    "        \n",
    "        # print(\"l1out.shape: After C1 and TanH\", l1out.shape)\n",
    "\n",
    "        # AvgPooling\n",
    "        l2out = self.S2(l1out)\n",
    "        \n",
    "        # print(\"l2out.shape: after AvgPooling \", l2out.shape)\n",
    "\n",
    "        l3out = self.C3(l2out)\n",
    "        l3out = torch.nn.Tanh()(l3out)\n",
    "        \n",
    "        # print(\"l3out.shape: After C3 and TanH\", l3out.shape)\n",
    "\n",
    "        # AvgPooling\n",
    "        l4out = self.S4(l3out)\n",
    "        \n",
    "        # print(\"l4out.shape, After Avg Pooling: \", l4out.shape)\n",
    "\n",
    "        l5out = self.C5(l4out)\n",
    "        l5out = torch.nn.Tanh()(l5out)\n",
    "        l5out = np.squeeze(l5out)\n",
    "        # print(\"l5out.shape: after C5 and Tanh: \", l5out.shape)\n",
    "        \n",
    "        l5out = torch.nn.Dropout()(l5out)\n",
    "        \n",
    "        l6out = self.F6(l5out) # squeeze bcs (120,1) vector to (120,) - a numpy broadcast thing\n",
    "        l6out = torch.nn.Tanh()(l6out)\n",
    "        # print(\"l6out.shape: after F6 and squeeze \", l6out.shape)\n",
    "        \n",
    "        l7out = self.F7(l6out)\n",
    "        # print(\"l7out.shape: After F7 \", l7out.shape)\n",
    "\n",
    "        return l7out\n",
    "    \n",
    "    \n",
    "#feature extraction C1\n",
    "#activation with Tanh\n",
    "#Pooling (reduce dimensions) S2\n",
    "\n",
    "#feature extraction C3\n",
    "#activation with Tanh\n",
    "#pooling (reduce dimensions) S4\n",
    "\n",
    "#feature extraction C5\n",
    "#activatiton with Tanh\n",
    "\n",
    "#Linear squeeze \n",
    "#activation with Tanh\n",
    "\n",
    "#Linear squeeze to outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4V-eaEc-4nS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Split Train into Train and Dev. Test set is in another HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZdPCiSh0-4nS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create Cross-Validation/Dev set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEo_mNQ9-4nT",
    "outputId": "62447745-daa2-4fe2-f697-83fd3b6910c1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  torch.Size([864, 64, 64, 3])\n",
      "y_train.shape:  torch.Size([864])\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train.shape: \", X_train.shape)\n",
    "print(\"y_train.shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XQQxpBft-4nU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODE = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2RHeXZPe-4nU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/installing-pytorch-on-apple-m1-chip-with-gpu-acceleration-3351dc44d67c\n",
    "\n",
    "# this ensures that the current MacOS version is at least 12.3+\n",
    "#print(torch.backends.mps.is_available())\n",
    "\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "#print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8d__kPsk-4nV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='mps')"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.device(\"mps\") # Disable this if not on apple silicon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "f2UVFdKLFjQl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_IMAGES = X_train.to(device)\n",
    "TRAIN_LABELS = y_train.to(device)\n",
    "DEV_IMAGES = X_dev.to(device)\n",
    "DEV_LABELS = y_dev.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNQjsoGxLQMw",
    "outputId": "25dd1509-e1f4-49fa-9e91-1bbd1e8b7471",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_IMAGES.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rKw4VH_aQatA",
    "outputId": "645e1802-bd89-4c59-c4cf-3dc614eda42a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([864, 64, 64, 3])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_IMAGES.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "G43CZmbqQdrS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_IMAGES = TRAIN_IMAGES.view(len(TRAIN_IMAGES), 3, 64, 64)\n",
    "DEV_IMAGES = DEV_IMAGES.view(len(DEV_IMAGES), 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YuIQKyBY-4nW",
    "outputId": "782eedc1-be33-4e21-8ddf-72bc8144fbc2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On step 0:\tTrain loss 1.7912440299987793\t|\tDev acc is 0.15740740299224854\n",
      "On step 100:\tTrain loss 0.9598339200019836\t|\tDev acc is 0.6064814925193787\n",
      "On step 200:\tTrain loss 0.6749681830406189\t|\tDev acc is 0.6574074029922485\n",
      "On step 300:\tTrain loss 0.5459597110748291\t|\tDev acc is 0.6759259104728699\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [19], line 58\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# Zero gradients, perform a backward pass, and update the weights.\u001B[39;00m\n\u001B[1;32m     57\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 58\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     61\u001B[0m train_acc, train_loss \u001B[38;5;241m=\u001B[39m approx_train_acc_and_loss(model, train_imgs, TRAIN_LABELS)\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/autograd/__init__.py:204\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    199\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    201\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 204\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    205\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from utils.accuracies import dev_acc_and_loss, approx_train_acc_and_loss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Modified From Ariel's EN.601.675 HW3\n",
    "        \n",
    "LOG_DIR = \".\"\n",
    "MODEL_SAVE_DIR = \".\"\n",
    "LEARNING_RATE = 0.0004 #arguments.get('lr')\n",
    "BATCH_SIZE = 100 # 250 #arguments.get('bs')\n",
    "EPOCHS = 2000 # arguments.get('epochs')\n",
    "DATE_PREFIX = datetime.datetime.now().strftime('%Y%m%d%H%M')\n",
    "\n",
    "### format dataset to the appropriate shape/dimensions necessary to be input into the model\n",
    "\n",
    "\n",
    "n_train_imgs = TRAIN_IMAGES.shape[0]\n",
    "HEIGHT = TRAIN_IMAGES.shape[1]\n",
    "WIDTH = TRAIN_IMAGES.shape[2]\n",
    "#  this will not be correct if not all classes are present in training\n",
    "#   But if classes are entirely missing from training, we cannot possibly hope to do well on them\n",
    "N_CLASSES = len(torch.unique(TRAIN_LABELS))\n",
    "# raise NotImplementedError\n",
    "\n",
    "### Normalize \n",
    "train_imgs = (TRAIN_IMAGES - torch.mean(torch.Tensor.float(TRAIN_IMAGES)))/ torch.std(torch.Tensor.float(TRAIN_IMAGES))\n",
    "dev_imgs = (DEV_IMAGES - torch.mean(torch.Tensor.float(DEV_IMAGES)))/ torch.std(torch.Tensor.float(DEV_IMAGES))\n",
    "\n",
    "### change depending on your model's instantiation\n",
    "\n",
    "model = LeNet5(input_height = HEIGHT, input_width= WIDTH,\n",
    "                  n_classes=N_CLASSES)\n",
    "if device != torch.device(\"cpu\"):\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "### (OPTIONAL) : you can change the choice of optimizer here if you wish.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "train_acc_col = []\n",
    "train_loss_col = []\n",
    "dev_acc_col = []\n",
    "dev_loss_col = []\n",
    "steps_col = []\n",
    "for step in range(EPOCHS):\n",
    "    i = np.random.choice(train_imgs.shape[0], size=BATCH_SIZE, replace=False)\n",
    "    x = train_imgs[i]#).astype(np.float32))\n",
    "    y = TRAIN_LABELS[i]#.astype(int)))\n",
    "\n",
    "    # x = x.unsqueeze(0)\n",
    "\n",
    "    # Forward pass: Get logits for x\n",
    "    logits = model(x.view((100, 3, 64, 64)))\n",
    "    # Compute loss\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc, train_loss = approx_train_acc_and_loss(model, train_imgs, TRAIN_LABELS)\n",
    "    dev_acc, dev_loss = dev_acc_and_loss(model, dev_imgs, DEV_LABELS)\n",
    "    train_acc_col.append(train_acc)\n",
    "    train_loss_col.append(train_loss)\n",
    "    dev_acc_col.append(dev_acc)\n",
    "    dev_loss_col.append(dev_loss)\n",
    "    steps_col.append(step)\n",
    "    # log model performance every 100 epochs\n",
    "    if step % 100 == 0:\n",
    "      \n",
    "        step_metrics = {\n",
    "            'step': step, \n",
    "            'train_loss': loss.item(), \n",
    "            'train_acc': train_acc,\n",
    "            'dev_loss': dev_loss,\n",
    "            'dev_acc': dev_acc\n",
    "        }\n",
    "\n",
    "        print(f\"On step {step}:\\tTrain loss {train_loss}\\t|\\tDev acc is {dev_acc}\")\n",
    "        # logger.writerow(step_metrics)\n",
    "# LOGFILE.close()\n",
    "\n",
    "# Save model\n",
    "model_savepath = os.path.join(MODEL_SAVE_DIR,f\"{DATE_PREFIX}_bestmodel.pt\")\n",
    "\n",
    "\n",
    "print(\"Training completed, saving model at {model_savepath}\")\n",
    "torch.save(model, model_savepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mM9hLVdSJusI",
    "outputId": "17925a7f-f3c4-4771-b6a4-1368294bba58",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "next(model.parameters()).device # Where is model on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "id": "45tiJ6JOJhLJ",
    "outputId": "74a238f5-1c1e-4e15-a8e3-382be79da997",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.reshape(x,(0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jCL1mH_-4nX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(steps_col, train_acc_col, label = \"Training Accuracy\")\n",
    "plt.plot(steps_col, dev_acc_col, label = \"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "fig2 = plt.figure()\n",
    "plt.plot(steps_col, train_loss_col, label = \"Training Loss\")\n",
    "plt.plot(steps_col, dev_loss_col, label = \"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Stats:\\tTrain loss {train_loss_col[1999]}\\t|\\tValidation Loss {dev_loss_col[1999]}\\t|\\tTrain acc is {train_acc_col[1999]} \\t|\\tDev acc is {dev_acc_col[1999]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpcQDoJP-4nY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Let's see performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAicPKsc-4nY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_data = h5py.File(\"./datasets/Signs_Data_Testing.h5\", \"r\")\n",
    "TEST_IMAGES = test_data[\"test_set_x\"]\n",
    "test_labels = test_data[\"test_set_y\"] # Do not train on these!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEFgf4JX-4nZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Transpose into correct shape for PyTorch\n",
    "\n",
    "TEST_IMAGES = np.transpose(TEST_IMAGES, axes=(0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SomM_LdF-4nZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TEST_IMAGES.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZoyVolm-4na",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODE = \"test\"\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WaoXYF-b-4nb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVQEdjLR-4nb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model = torch.load(WEIGHTS_FILE) # Load model parameters from file to avoid retraining\n",
    "\n",
    "predictions = []\n",
    "# testSetMean = TEST_IMAGES.mean() # Can't do that - only remove training parameters from it\n",
    "# testSetStd = TEST_IMAGES.std()\n",
    "for test_case in TEST_IMAGES:\n",
    "\n",
    "    ### normalization schemes\n",
    "    test_case = (test_case - np.mean(TRAIN_IMAGES)) / np.std(TRAIN_IMAGES)\n",
    "\n",
    "    # test_case = test_case.reshape(1,28,28)\n",
    "\n",
    "\n",
    "    x = torch.from_numpy(test_case.astype(np.float32))\n",
    "    # x = x.view(1,-1)\n",
    "    logits = model(x.unsqueeze(0))\n",
    "    logits = logits.reshape(-1, 6)\n",
    "    pred = torch.max(logits, 1)[1]\n",
    "    predictions.append(pred.item())\n",
    "\n",
    "predictions[:15] # First 15 predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68FWQRcU-4nc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_4y-zNi-4nd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils.accuracies import accuracy\n",
    "\n",
    "print(\"Test accuracy: \", accuracy(np.array(predictions), np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1iW7rv-I-4nd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "transform = A.Compose([\n",
    "    A.affinerotate(p = 0.33)\n",
    "    #A.HorizontalFlip(p = 0.5),\n",
    "    #A.ColorJitter(brightness= 0, contrast=(1, 1.5), saturation=(0.5,1), hue=0, p=1)\n",
    "])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uD9xR9jM-4nf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "X_dev2 = []\n",
    "for num in range(3):\n",
    "    for pos, imageSet in enumerate(X_dev):\n",
    "        merged = cv2.merge([imageSet[2], imageSet[1], imageSet[0]])\n",
    "        image = cv2.cvtColor(merged, cv2.COLOR_BGR2RGB)\n",
    "        image = transform(image=image)['image']\n",
    "        X_dev2.append(image)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGtPeYsH-4ng",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for pos, image in enumerate(X_dev2):\n",
    "    plt.figure()\n",
    "    plt.title(pos)\n",
    "    plt.imshow(image)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5KvAsFR-4nh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for pos, imageSet in enumerate(X_dev):\n",
    "    if pos == 87:\n",
    "        merged = cv2.merge([imageSet[2], imageSet[1], imageSet[0]])\n",
    "        image = cv2.cvtColor(merged, cv2.COLOR_BGR2RGB)\n",
    "        plt.figure()\n",
    "        plt.title(pos)\n",
    "        plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DejP_-mJ-4ni",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(y_dev[87])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yd0ixj9x-4nj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}